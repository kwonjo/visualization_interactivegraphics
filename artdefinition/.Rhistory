library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(topicmodels)
# Read the text file
setwd("/Users/jkwon/Desktop/Jo2018/2018interactivegraphics/p5_visualization/artdefinition")
text <- readLines("art.txt", warn=F)
docs<-Corpus(VectorSource(text))
inspect(docs)
# Remove punctuation, numbers
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
inspect(docs)
dtm <- DocumentTermMatrix(docs)
##Run Latent Dirichlet Allocation (LDA) using Gibbs Sampling
#set burn in
burnin <-1000
#set iterations
iter<-2000
#thin the spaces between samples
thin <- 500
#set random starts at 5
nstart <-5
#use random integers as seed
seed <- list(254672,109,122887,145629037,3)
# return the highest probability as the result
best <-TRUE
#set number of topics
k <-5
#run the LDA model
ldaOut <- LDA(dtm, k, method="Gibbs", control=
list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
terms(ldaOut, 10)
ldaOut.terms <- as.matrix(terms(ldaOut,10))
#view the topic assignment for each document
topics(ldaOut)
#create a matrix and write to csv
ldaOut.topics <-as.matrix(topics(ldaOut))
write.csv(ldaOut.topics,file=paste("LDAGibbs",k,"DocsToTopics.csv"))
#Find probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("LDAGibbs",k,"TopicProbabilities.csv"))
#investigate topic probabilities data.frame
summary(topicProbabilities)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(topicmodels)
setwd("/Users/jkwon/Desktop/Jo2018/2018interactivegraphics/p5_visualization/artdefinition")
text <- readLines("art.txt", warn=F)
docs<-Corpus(VectorSource(text))
inspect(docs)
# Remove punctuation, numbers
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
# Remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
inspect(docs)
dtm <- DocumentTermMatrix(docs)
##Run Latent Dirichlet Allocation (LDA) using Gibbs Sampling
#set burn in
burnin <-1000
#set iterations
iter<-2000
#thin the spaces between samples
thin <- 500
#set random starts at 5
nstart <-5
#use random integers as seed
seed <- list(254672,109,122887,145629037,3)
# return the highest probability as the result
best <-TRUE
#set number of topics
k <-5
#run the LDA model
ldaOut <- LDA(dtm, k, method="Gibbs", control=
list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
terms(ldaOut, 20)
ldaOut.terms <- as.matrix(terms(ldaOut,10))
#view the topic assignment for each document
topics(ldaOut)
#create a matrix and write to csv
ldaOut.topics <-as.matrix(topics(ldaOut))
write.csv(ldaOut.topics,file=paste("LDAGibbs",k,"DocsToTopics.csv"))
#Find probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("LDAGibbs",k,"TopicProbabilities.csv"))
#investigate topic probabilities data.frame
summary(topicProbabilities)
str(topicProbabilities)
View(ldaOut)
